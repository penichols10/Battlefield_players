{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import sqlite3\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach: Use Selenium to Load JSON\n",
    "First, a quick discussion of the [website](https://battlefieldtracker.com/bfv/leaderboards/stats/all/Wins?type=stats&page=1) layout. The previous link is to the first leaderboard page sorted by the number of games users have won. This does not in and of itself provide us with much information. But you can click on each user on the leaderboard and see an associated profile. This profile contains a lot of useful information. We would like to collect this information for every user on every page of the leaderboard.\n",
    "\n",
    "The data on the website is loaded using Javascript. This means that simply sending GET requests and parsing HTML using Python's request library will not work - the HTML will not contain the desired data as it will not have been loaded. A plausible approach is to just use Selenium to load the leaderboard pages, get the HTML once data has been loaded onto the page with Javascript, find profile URLs from the leaderboard page HTML, load the profile pages with Selenium, then go through the profile page HTML to find data. There are two problems with this approach, both associated with page loading times. First, page loading times vary. To scrape a dynamic page in this fashion, first Selenium starts loading the page. Then the program waits for some set time. After waiting, Selenium passes the loaded page's HTML to BeautifulSoup for parsing. But if the page isn't loaded, the program could crash or at best there will be missing data. To prevent this, the program has to wait for several seconds for pages to load, which becomes prohibitively slow to scrape the whole site. Second, the pages just generally load slowly. This method would have taken several days even with a reasonably low loss rate of around 1% of the data.\n",
    "\n",
    "A second approach requires determining if the website gets its data from an API. If it does, you may use the API to load the data in JSON format. The advantages here are that you do not have to spend time loading the actual pages, there is data in the API that is not loaded on the page, and the API stores data in a more human readable format than on in the HTML. To find the API, load the page in a browser (I use Firefox for this). Open the developer tools panel with the F12 key. Click the Network tab on the panel and the XHR button on the row below that. Reload the page. The list should populate, and if you look for requests with Type JSON, you should eventually find the API urls. You can use a tool like Insomnia to quickly generate code to make requests to the API with Python. The problem in this case is the APIs did not accept requests.\n",
    "\n",
    "The third and final approach combines the previous. I use the API, but I use Selenium to load the response to API requests in the browser rather than the pages that display data from the API. These are in JSON format and load nearly instantly. This eliminates or reduces the downsides of the first approach (slow, lost data) and retains most of the benefits of the second approach (faster, no lost data, more features). For scale, this probably speeds up the scraping process by about an order of magnitude, with no lost data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leaderboard\n",
    "The first step is to get data from the leaderboard page itself. Since so much data can be retrived from the API that loads the profile pages, all that is needed from the leaderboard are the names of players and platforms they play on. The players usernames and platforms are combined to make a features I call \"player_id.\" These take the form (platform_abbreviation)/(username). In addition to serving as identifiers for players, they are needed to get unique information about players from the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = {}\n",
    "def parse_player(player_json):\n",
    "    '''\n",
    "    Retrives the players username and platform and returns their player_id and platform\n",
    "    Parameters: player_json, the portion of a JSON string from the leaderboard API associated with a player.\n",
    "    Returns: player_id, player_platform\n",
    "    '''\n",
    "    player_username = player_json['id']\n",
    "    player_platform = player_json['owner']['metadata']['platformSlug']\n",
    "    player_id = f'{player_platform}/{player_username}'\n",
    "    \n",
    "    return player_id, player_platform\n",
    "\n",
    "def parse_leaderboard(leaderboard_url, stat_dict):\n",
    "    '''\n",
    "    Retrives the player_id and player_platform for every play on a leaderboard page\n",
    "    Parameters: leaderboard_url (url for the leaderboard API for a page), stat_dict (dictionary of player performance metrics and other player information)\n",
    "    '''\n",
    "\n",
    "    # Load the URL\n",
    "    driver.get(leaderboard_url)\n",
    "    leaderboard_content = driver.page_source.encode('utf-8').strip()\n",
    "    soup = bs(leaderboard_content)\n",
    "    leaderboard_json = json.loads(soup.body.pre.text)\n",
    "\n",
    "    if 'data' in leaderboard_json:\n",
    "        leaderboard_data_json = leaderboard_json['data']['items']\n",
    "        # Get names from leaderboard\n",
    "        for player_json in leaderboard_data_json:\n",
    "            player_id, player_platform = parse_player(player_json)\n",
    "            # Add the player_id to the dictionary\n",
    "            if 'player_id' in stat_dict:\n",
    "                stat_dict['player_id'].append(player_id)\n",
    "            else:\n",
    "                stat_dict['player_id'] = [player_id]\n",
    "\n",
    "            # Add the player platform to the dictionary\n",
    "            if 'platform' in stat_dict:\n",
    "                stat_dict['platform'].append(player_platform)\n",
    "            else:\n",
    "                stat_dict['platform'] = [player_platform]\n",
    "\n",
    "    else:\n",
    "        if 'player_id' not in stat_dict:\n",
    "            stat_dict['player_id'] = []\n",
    "            stat_dict['platform'] = []\n",
    "            \n",
    "    return stat_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profile\n",
    "The second step is to scrape data from the player profiles. The profile consists of two broad categories of data. First, overall information, which is referred to as player \"history.\" These are metric calculated accross class like kills, score, etc. Second, there are specific metrics broken out by class and vehicle use. All of these are stored in easy to navigate JSON strings, but they each require different API requests. In both categories, stats are given in raw format (ex. 10000 in the score feature means the player's total score while playing has been 10000) and percentile format (ex. 1.9 in the score feature means the player is in the 1.9% of highest scorign players). These are given different suffixes - _value and _score - to make them identifiable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall History\n",
    "The first thing I retrieve is player history information. This is a simple matter of looping over different performance metrics and storing information in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_history_for_player(history_json, stat_dict, history_categories):\n",
    "    available_stats = history_json.keys()\n",
    "    for stat in history_categories:\n",
    "        if stat in available_stats:\n",
    "            stat_value = history_json[stat]['value']\n",
    "            stat_percentile = history_json[stat]['percentile']\n",
    "        else:\n",
    "            stat_value = np.nan\n",
    "            stat_percentile = np.nan\n",
    "        \n",
    "        # Add value to stat_dict\n",
    "        if stat+'_value' in stat_dict:\n",
    "            stat_dict[stat+'_value'].append(stat_value)\n",
    "        else:\n",
    "            stat_dict[stat+'_value'] = [stat_value]\n",
    "            \n",
    "        # Add percentile to stat_dict\n",
    "        if stat+'_percentile' in stat_dict:\n",
    "            stat_dict[stat+'_percentile'].append(stat_percentile)\n",
    "        else:\n",
    "            stat_dict[stat+'_percentile'] = [stat_percentile]\n",
    "\n",
    "    return stat_dict  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class stats\n",
    "The first thing I retrieve is player class information. This is a simple matter of looping over different performance metrics and storing information in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_class_stats(class_json, stat_dict, class_categories):\n",
    "    '''\n",
    "    Retrieve class-specific stats for a specific player for a single class.\n",
    "    '''\n",
    "    class_name = class_json['metadata']['name']\n",
    "    available_stats = list(class_json['stats'].keys())[1:]\n",
    "\n",
    "    # Add class stats to stat_dict       \n",
    "    for stat in class_categories: # The first entry is player rank, which we don't need\n",
    "        # Check if the desired stat is present in the JSON\n",
    "        if stat in available_stats:\n",
    "            stat_percentile = class_json['stats'][stat]['percentile']\n",
    "            stat_value = class_json['stats'][stat]['value']\n",
    "        else:\n",
    "            stat_percentile = np.nan\n",
    "            stat_value = np.nan\n",
    "        \n",
    "        stat_name = f'{class_name}_{stat}' # ex Assault_kills\n",
    "        \n",
    "        # Add stat value to dictionary\n",
    "        if stat_name+'_value' in stat_dict:\n",
    "            stat_dict[stat_name+'_value'].append(stat_value)\n",
    "        else:\n",
    "            stat_dict[stat_name+'_value'] = [stat_value]\n",
    "        \n",
    "        # Add stat percentile to dictionary\n",
    "        if stat_name+'_percentile' in stat_dict:\n",
    "            stat_dict[stat_name+'_percentile'].append(stat_percentile)\n",
    "        else:\n",
    "            stat_dict[stat_name+'_percentile'] = [stat_percentile]\n",
    "            \n",
    "    return stat_dict\n",
    "\n",
    "def parse_classes_for_player(classes_json, stat_dict, class_categories):\n",
    "    '''\n",
    "    Retrieve class-specific stats for a specific player for all classes.\n",
    "    '''\n",
    "    classes = ['medic', 'assault', 'support', 'recon', 'tanker', 'pilot']\n",
    "    for class_json in classes_json:\n",
    "        stat_dict = parse_class_stats(class_json, stat_dict, class_categories)\n",
    "    \n",
    "    # Check to see if class data was found for all classes for the player - some players do not have data for certain classes\n",
    "    # First, identify class related features\n",
    "    class_features = []\n",
    "    for player_class in classes:\n",
    "        for feature in stat_dict:\n",
    "            if player_class in feature.lower():\n",
    "                class_features.append(feature)\n",
    "    \n",
    "    # Check to see if all class related features are of same length, if not, fill short features with NaN      \n",
    "    feature_lengths = [len(stat_dict[feature]) for feature in class_features]\n",
    "    unique_feature_lengths = set(feature_lengths)\n",
    "    if len(unique_feature_lengths) > 1:\n",
    "        num_samples = max(unique_feature_lengths)\n",
    "        for i, feature_length in enumerate(feature_lengths):\n",
    "            if feature_length < num_samples:\n",
    "                short_feature = class_features[i]\n",
    "                stat_dict[short_feature].append(np.nan) # Since we do this for each player, should never need to add more than 1 NaN per player\n",
    "                    \n",
    "    return stat_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Code and Automate Browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_player_stats(stat_dict, history_categories, class_categories):\n",
    "    for player_id in tqdm(stat_dict['player_id']):\n",
    "        \n",
    "        # Get overall history for player\n",
    "        api_url = f\"https://api.tracker.gg/api/v2/bfv/standard/profile/{player_id}?\"\n",
    "\n",
    "        driver.get(api_url)\n",
    "        history_content = driver.page_source.encode('utf-8').strip()\n",
    "        soup = bs(history_content)\n",
    "        history_json = json.loads(soup.body.pre.text)\n",
    "\n",
    "        # Some data is simply unavailable for access\n",
    "        if 'data' in history_json:\n",
    "            history_json_data = history_json['data']['segments'][0]['stats']\n",
    "\n",
    "            stat_dict = parse_history_for_player(history_json_data, stat_dict, history_categories)\n",
    "               \n",
    "        # Get class info for the user in question\n",
    "        api_url = f\"https://api.tracker.gg/api/v2/bfv/standard/profile/{player_id}/segments/class\"\n",
    "\n",
    "        driver.get(api_url)\n",
    "        class_content = driver.page_source.encode('utf-8').strip()\n",
    "        soup = bs(class_content)\n",
    "\n",
    "        classes_json = json.loads(soup.body.pre.text)\n",
    "        if 'data' in classes_json:\n",
    "            classes_json_data = classes_json['data']\n",
    "            \n",
    "            stat_dict = parse_classes_for_player(classes_json_data, stat_dict, class_categories)\n",
    "\n",
    "        # Drop player if no associated information\n",
    "        if 'data' not in history_json and 'data' not in classes_json:\n",
    "            stat_dict['player_id'] = stat_dict['player_id'][:-1]\n",
    "            stat_dict['platform'] = stat_dict['platform'][:-1]\n",
    "        \n",
    "        time.sleep(2)\n",
    "        \n",
    "        \n",
    "    return stat_dict        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sqlite3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Patrick\\Documents\\worth_keeping\\bfv_players\\Battlefield_players\\scraper_explanation.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 56>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Patrick/Documents/worth_keeping/bfv_players/Battlefield_players/scraper_explanation.ipynb#X14sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m             current_iter\u001b[39m.\u001b[39mto_sql(\u001b[39m'\u001b[39m\u001b[39mbfvstats\u001b[39m\u001b[39m'\u001b[39m, con\u001b[39m=\u001b[39mcon, if_exists\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mappend\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Patrick/Documents/worth_keeping/bfv_players/Battlefield_players/scraper_explanation.ipynb#X14sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m         skip \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m100\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Patrick/Documents/worth_keeping/bfv_players/Battlefield_players/scraper_explanation.ipynb#X14sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m con \u001b[39m=\u001b[39m sqlite3\u001b[39m.\u001b[39mconnect(\u001b[39m'\u001b[39m\u001b[39mbfvstats.db\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Patrick/Documents/worth_keeping/bfv_players/Battlefield_players/scraper_explanation.ipynb#X14sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m cur \u001b[39m=\u001b[39m con\u001b[39m.\u001b[39mcursor()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Patrick/Documents/worth_keeping/bfv_players/Battlefield_players/scraper_explanation.ipynb#X14sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m options \u001b[39m=\u001b[39m Options()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sqlite3' is not defined"
     ]
    }
   ],
   "source": [
    "def read_categories(categories_to_scrape_file):\n",
    "    with open(categories_to_scrape_file, 'r') as f:\n",
    "        categories = f.read().split('\\n')\n",
    "        history_categories = categories[0].split(' ')\n",
    "        class_categories = categories[1].split(' ')\n",
    "    return history_categories, class_categories\n",
    "\n",
    "def scrape_page(leaderboard_url, stat_dict, history_categories, class_categories):\n",
    "    # Scrape the page\n",
    "    stat_dict = parse_leaderboard(leaderboard_url, stat_dict)\n",
    "    stat_dict = parse_player_stats(stat_dict, history_categories, class_categories)\n",
    "    \n",
    "    return stat_dict\n",
    "\n",
    "def scrape_site(history_categories, class_categories):\n",
    "    # determine how many profiles to skip\n",
    "    skip = 0\n",
    "    files = os.listdir('data')\n",
    "    if len(files) > 0:\n",
    "        file_nums = map(lambda x: int(x.split('p')[1].split('.')[0]), files)\n",
    "        skip = max(list(file_nums)) + 100\n",
    "        print(f'Skipping first {skip} profiles.')\n",
    "\n",
    "    while skip < 78800:\n",
    "        leaderboard_url = f'https://api.tracker.gg/api/v1/bfv/standard/leaderboards?type=stats&platform=all&board=WINS&skip={skip}&take=100'\n",
    "        \n",
    "        # scrape page\n",
    "        stat_dict = {}\n",
    "        stat_dict = scrape_page(leaderboard_url, stat_dict, history_categories, class_categories)\n",
    "        key_lens = [len(stat_dict[key]) for key in stat_dict]\n",
    "        if len(list(set(key_lens))) > 1:\n",
    "            for key in stat_dict:\n",
    "                print(key, len(stat_dict[key]))\n",
    "\n",
    "\n",
    "        # Load previous progess, if any\n",
    "        current_iter = pd.DataFrame.from_dict(stat_dict)\n",
    "        previous_file_name = f'bfvstats_skip{skip-100}.csv'\n",
    "        \n",
    "        if previous_file_name in files:\n",
    "            previous_iter = pd.read_csv('data/'+previous_file_name, index_col=0)\n",
    "            combined_df = pd.concat([previous_iter, current_iter]).reset_index(drop=True)\n",
    "            combined_df = combined_df.drop_duplicates(subset=['player_id'])\n",
    "\n",
    "            # os.remove(previous_file_name) T\n",
    "            combined_df.to_csv(f'data/bfvstats_skip{skip}.csv')\n",
    "            current_iter.to_sql('bfvstats', con=con, if_exists='append')\n",
    "                     \n",
    "            \n",
    "        else:\n",
    "            current_iter.to_csv(f'data/bfvstats_skip{skip}.csv')\n",
    "            current_iter.to_sql('bfvstats', con=con, if_exists='append')\n",
    "\n",
    "        skip += 100\n",
    "\n",
    "con = sqlite3.connect('bfvstats.db')\n",
    "cur = con.cursor()\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"start-maximized\")\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=options)\n",
    "history_categories, class_categories = read_categories('categories_to_scrape.txt')\n",
    "\n",
    "scrape_site(history_categories, class_categories)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
